{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Azure storage access info\n",
        "blob_account_name = \"covidtrackingdefinitive\"\n",
        "blob_container_name = \"public\"\n",
        "blob_relative_path = \"curated/covid-19/covid_definitiveHC/latest/covid_tracking.csv\"\n",
        "blob_sas_token = r\"sp=r&st=2020-09-07T07:40:15Z&se=2020-09-08T15:40:15Z&spr=https&sv=2019-12-12&sr=b&sig=%2F1%2FjjFQu5x7A3TC3MLDCDoAddt%2BIUlBeKTb%2FU1yobsI%3D\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Allow SPARK to read from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "spark.conf.set(\n",
        "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
        "  blob_sas_token)\n",
        "print('Remote blob path: ' + wasbs_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# SPARK read parquet, note that it won't load any data yet by now\n",
        "df = spark.read.load(wasbs_path, format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
        "print('Register the DataFrame as a SQL temporary view: source')\n",
        "df.createOrReplaceTempView('source')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "Load_Date"
            ],
            "values": [
              "Row_ID"
            ],
            "yLabel": "Row_ID",
            "xLabel": "Load_Date",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"Row_ID\":{\"2020-07-16T00:00:00.000Z\":55}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "# Display top 10 rows\n",
        "print('Displaying top 10 rows: ')\n",
        "display(spark.sql('SELECT * FROM source LIMIT 10'))\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS Covid19\")\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"Covid19.definitiveHealthCare\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [
              "count(1)"
            ],
            "yLabel": "count(1)",
            "xLabel": "",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"count(1)\":{\"\":2000}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "#View the Count of records that have been added to the table.\n",
        "display(spark.sql(\"SELECT count(*) FROM covid19.definitivehealthcare\"))"
      ],
      "attachments": {}
    }
  ]
}